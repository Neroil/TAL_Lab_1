{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# TAL Labo 1b : Segmentation de textes avec NLTK\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Le but de cette deuxième partie du Labo 1 du Cours TAL est d'effectuer quelques opérations élémentaires sur les textes en utilisant la boîte à outils [NLTK](http://www.nltk.org/) en Python.  Vous utiliserez l'environnement mis en place dans la partie 1a : [Python 3](https://www.python.org/) avec _notebooks_ [Jupyter](https://jupyter.org/), soit localement sur votre ordinateur (avec ou sans Conda) ou en ligne sur [Google Colab](https://colab.research.google.com).\n",
    "\n",
    "Vous utiliserez NLTK pour obtenir des textes en ligne ou localement, puis vous les segmenterez en phrases et en mots (appelés aussi _tokens_).  Vous calculerez aussi des statistiques sur ces textes.  Vous travaillerez d'abord sur des textes en anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK: Natural Language Toolkit\n",
    "\n",
    "Pour ajouter NLTK à votre installation locale de Python, suivez les instructions sur le [site web NLTK](http://www.nltk.org/install.html).  Sur Google Colab, NLTK est déjà installé.\n",
    "\n",
    "Rappels de Python: pour utiliser NLTK en Python, notamment dans un _notebook_ Jupyter, vous devez exécuter `import nltk`, ce qui vous permet d'accéder à toutes les commandes avec le préfixe `nltk`.  Si vous écrivez `from nltk.book import *`, cela importera des commandes et des variables (p.ex. une collection de textes) qui seront accessibles sans utiliser le préfixe. \n",
    "\n",
    "**Remarques**\n",
    "* Le but de ce laboratoire est de vous initier à NLTK.  Vous pouvez aussi parcourir le [Chapitre 1](http://www.nltk.org/book/ch01.html) du [livre NLTK (*NLP with Python*)](http://www.nltk.org/book/) et essayer les commandes indiquées.  \n",
    "* Veuillez noter que le [livre en ligne](http://www.nltk.org/book/) est mis à jour pour Python 3, mais la [version imprimée](http://shop.oreilly.com/product/9780596516499.do) que l'on peut parfois trouver en PDF est pour Python 2. \n",
    "* NLTK inclut un gestionnaire de téléchargements (faire `import nltk` puis `nltk.download()`).  Cela vous permettra de télécharger de nombreux corpus additionnels, mais que nous n'utiliserons pas dans ce laboratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\massi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "#from nltk.book import *\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Écrivez une liste de mots (strings) que vous appelerez `sentence1`, formant une phrase en anglais.\n",
    "\n",
    "Affichez sa longueur avec `len()`.  \n",
    "\n",
    "Utilisez `nltk.bigrams` pour générer tous les bi-grammes (couples de mots adjacents) à partir de cette liste.  Un exemple est montré dans la [section 3.3 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#collocations-and-bigrams).  \n",
    "\n",
    "Enfin, triez les bi-grammes par ordre alphabétique et affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[('and', 'fries'), ('cheeseburgers', 'and'), ('i', 'love'), ('love', 'cheeseburgers')]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "sentence1 = [\"i\", \"love\", \"cheeseburgers\", \"and\", \"fries\"]\n",
    "\n",
    "print(len(sentence1))\n",
    "\n",
    "bigrams = nltk.bigrams(sentence1)\n",
    "print(sorted(list(bigrams)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Définissez maintenant une chaîne unique appelée `string2` contenant une phrase, incluant par exemple des ponctuations.  \n",
    "\n",
    "Utilisez le *tokenizer* de NLTK (fonction `nltk.word_tokenize`, expliquée dans la [section 3.1 du livre NLTK](http://www.nltk.org/book/ch03.html#sec-accessing-text)) pour segmenter la chaîne en mots, qui seront rassemblés dans une liste que vous appelerez `sentence2`.  Affichez cette liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'cheeseburgers', 'and', 'fries', '!']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "string2 = \"I love cheeseburgers and fries !\"\n",
    "sentence2 = nltk.word_tokenize(string2)\n",
    "\n",
    "print(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utiliser NLTK pour télécharger, segmenter et sauvegarder un texte\n",
    "\n",
    "**2a.** Inspirez-vous du [chapitre 3 (3.1. Processing Raw Text) du livre NLTK](http://www.nltk.org/book/ch03.html) pour télécharger un fichier avec du texte en ligne, par exemple un livre anglais du Projet Gutenberg.  Stockez son contenu dans une chaîne et affichez sa longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)\n",
    "len(raw)\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** En inspectant la chaîne précédente, déterminez quelle partie du début et de la fin il faut enlever pour garder uniquement le texte principal du roman.  Vous pouvez utiliser la notation dite de *slicing* en Python, et essayer de localiser des chaînes qui indiquent où commence et finit le véritable texte. Quelle est la longueur de ce texte en caractères ?  \n",
    "\n",
    "Si vous avez des problèmes avec les retours à la ligne, pensez à les remplacer avec la fonction `.replace(..., ...)`.  Si vous avez des problèmes d'encodage, voir le [support d'Unicode dans Python](https://docs.python.org/3.7/howto/unicode.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n",
      "1135166\n",
      "PART I\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "On an exceptionally hot eveni\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'of a new story, but our present story is\\nended.\\n\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "indexBegin = raw.find('PART I')\n",
    "print(indexBegin)\n",
    "indexEnd = raw.find('*** END OF')\n",
    "print(indexEnd)\n",
    "\n",
    "\n",
    "interestingText = raw[indexBegin:indexEnd]\n",
    "\n",
    "#Check si le début est bien dedans !\n",
    "print(interestingText[:50])\n",
    "\n",
    "#Check si la fin est bien dedans !\n",
    "interestingText[len(interestingText) - 50:] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Veuillez maintenant segmenter le texte en phrases avec NLTK, et afficher le nombre de phrases obtenues, ainsi qu'un court extrait de 4 phrases. \n",
    "\n",
    "Puis veuillez écrire les phrases dans un nouveau fichier, avec une phrase par ligne.\n",
    "\n",
    "Comment appréciez-vous la qualité de la segmentation ?  Veuillez écrire votre appréciation dans une nouvelle cellule ci-dessous.\n",
    "\n",
    "**Indications :**\n",
    "* si certains caractères spéciaux vous semblent péjorer la segmentation, vous pouvez les remplacer dans la chaîne globale avec la fonction `.replace('s1', 's2')`\n",
    "* vous aurez besoin de la fonction de nltk (vue en cours) appelée `nltk.sent_tokenize(...)` qui est documentée [ici](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) (rappel : le nom \"sentence tokenize\" n'est pas très logique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases tokenisées : 11903\n",
      "Extrait de 4 phrases tokenisées :\n",
      "Phrase 2 : He had successfully avoided meeting his landlady on the staircase.\n",
      "Phrase 3 : His garret was under the roof of a high, five-storied house and was more like a cupboard than a room.\n",
      "Phrase 4 : The landlady who provided him with garret, dinners, and attendance, lived on the floor below, and every time he went out he was obliged to pass her kitchen, the door of which invariably stood open.\n",
      "Phrase 5 : And each time he passed, the young man had a sick, frightened feeling, which made him scowl and feel ashamed.\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "# Enlever les \\n\n",
    "interestingTextCleaned = interestingText.replace('\\n', ' ')\n",
    "\n",
    "sentences = nltk.sent_tokenize(interestingTextCleaned)\n",
    "\n",
    "print(\"Nombre de phrases tokenisées :\", len(sentences))\n",
    "\n",
    "print(\"Extrait de 4 phrases tokenisées :\")\n",
    "for i, tokens in enumerate(sentences[1:5], start=2):\n",
    "    print(f\"Phrase {i} :\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"sample_text_1.txt\"\n",
    "# Pour un fichier local : chemin relatif par rapport au notebook\n",
    "# Pour Google Colab, p.ex.: /content/gdrive/My Drive/sample_text_1.txt\n",
    "if os.path.exists(filename1): \n",
    "    os.remove(filename1)\n",
    "\n",
    "\n",
    "with open(filename1, \"a\", encoding=\"utf8\") as fd:\n",
    "    fd.write(\"\\n\".join(sentences) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre appréciation de la qualité ici :\n",
    "## Ca m'a l'air OK, sauf lorsqu'il y a des parties ou des titres, ils se collent à la phrase suivante !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Veuillez maintenant segmenter chaque phrase du (2c) en tokens (mots et ponctuations).  Stockez le résultat dans une nouvelle variable (liste de listes).  Affichez le nombre total de _tokens_. Puis, affichez 4 phrases et commentez la qualité de la tokenisation.  Veuillez également créer un autre fichier, avec une phrase par ligne, et un espace entre chaque _token_.  \n",
    "\n",
    "Vous aurez besoin de la fonction de nltk (vue en cours) appelée `nltk.word_tokenize(...)` (documentée [ici](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de tokens (segmentation phrase par phrase) : 252755\n",
      "Extrait de 4 phrases tokenisées :\n",
      "Phrase 2 : ['He', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.']\n",
      "Phrase 3 : ['His', 'garret', 'was', 'under', 'the', 'roof', 'of', 'a', 'high', ',', 'five-storied', 'house', 'and', 'was', 'more', 'like', 'a', 'cupboard', 'than', 'a', 'room', '.']\n",
      "Phrase 4 : ['The', 'landlady', 'who', 'provided', 'him', 'with', 'garret', ',', 'dinners', ',', 'and', 'attendance', ',', 'lived', 'on', 'the', 'floor', 'below', ',', 'and', 'every', 'time', 'he', 'went', 'out', 'he', 'was', 'obliged', 'to', 'pass', 'her', 'kitchen', ',', 'the', 'door', 'of', 'which', 'invariably', 'stood', 'open', '.']\n",
      "Phrase 5 : ['And', 'each', 'time', 'he', 'passed', ',', 'the', 'young', 'man', 'had', 'a', 'sick', ',', 'frightened', 'feeling', ',', 'which', 'made', 'him', 'scowl', 'and', 'feel', 'ashamed', '.']\n",
      "[['PART', 'I', 'CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.'], ['He', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "total_tokens = sum(len(tokens) for tokens in tokenized_sentences)\n",
    "print(\"Nombre total de tokens (segmentation phrase par phrase) :\", total_tokens)\n",
    "\n",
    "# Affichage d'un extrait de 4 phrases tokenisées (par exemple, phrases 6 à 9)\n",
    "print(\"Extrait de 4 phrases tokenisées :\")\n",
    "for i, tokens in enumerate(tokenized_sentences[1:5], start=2):\n",
    "    print(f\"Phrase {i} :\", tokens)\n",
    "\n",
    "print(tokenized_sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = \"sample_text_2.txt\"\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "with open(filename2, \"w\", encoding=\"utf8\") as fd:\n",
    "    for tokens in tokenized_sentences:\n",
    "        fd.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre appréciation de la qualité ici :\n",
    "## C'est très bien, or les ponctuations ont été séparées des mots. C'est un peu bizarre, mais ça reste \"correct\".\n",
    "## Toutefois, on peut remarquer que les abréviations n'ont pas été séparées de leur point, ce qui est bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e.** Veuillez maintenant tokeniser un texte sans le segmenter préalablement en phrases, en effectuant la tokenisation directement sur la chaîne de caractères contenant tout le texte.  Veuillez afficher un court extrait de 50 tokens. \n",
    "\n",
    "Veuillez afficher le nombre total de tokens : est-ce qu'il est identique au total obtenu au (2d) ?  \n",
    "\n",
    "Il n'est pas demandé ici d'écrire le résultat dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrait de 50 tokens (tokenisation directe) : ['PART', 'I', 'CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase']\n",
      "Nombre total de tokens (tokenisation directe) : 252755\n",
      "Nombre total de tokens (segmentation phrase par phrase) : 252755\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "# 3a. Créer un objet nltk.Text à partir de la liste des tokens\n",
    "all_tokens = nltk.word_tokenize(interestingTextCleaned)\n",
    "print(\"Extrait de 50 tokens (tokenisation directe) :\", all_tokens[:50])\n",
    "print(\"Nombre total de tokens (tokenisation directe) :\", len(all_tokens))\n",
    "print(\"Nombre total de tokens (segmentation phrase par phrase) :\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse à la question ici :\n",
    "# Le nombre de tokens reste le même, mais on pert l'information des phrases. \n",
    "# C'est à dire, on ne sait plus où commence et où finit une phrase. On pourrait les retrouver avec la ponctuation.\n",
    "# Mais ça reste très bien!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculer diverses statistiques avec NLTK\n",
    "Pour calculer des statistiques, il faut d'abord créer un objet de type `nltk.Text`.  \n",
    "\n",
    "Les méthodes `nltk.word_tokenize()` et `nltk.sent_tokenize()` s'appliquent à des chaînes, pas des `nltk.Text`.\n",
    "\n",
    "Les objets `nltk.Text` peuvent en principe être créés avec : \n",
    "1. la chaîne de caractères constituant le texte (string)\n",
    "2. la liste de tous les mots du texte (tableau de string)\n",
    "3. la liste de toutes les phrases (tableau de listes de string)\n",
    "\n",
    "**Seule l'option (2) permet d'utiliser correctement les méthodes de `nltk.Text`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Créez et stockez dans une variable un objet `nltk.Text` à partir de la liste des tokens de votre texte.  Vous pouvez appeler ici de nouveau la fonction `nltk.word_tokenize` ou réutiliser le résultat du (2e).  Il n'y a rien à afficher ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "text = nltk.Text(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez afficher les occurrences d'un mot et leur contexte immédiat avec la méthode `concordance`.  Cette méthode est décrite au\n",
    " [chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html) qui montre aussi d'autres exemples d'opérations que l'on peut effectuer sur un objet `nltk.Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "beziatnikov , who felt a return of affection for Pyotr Petrovitch . “ And , wha\n",
      "sure that it was only your special affection for my poor husband that has made \n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "text.concordance(\"affection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Veuillez trouver les 10 mots qui ont les contextes les plus semblables à un mot de votre choix, en utilisant la méthode `similar`.  Cette méthode est aussi décrite au [chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html).  Est-ce que les mots ayant des contextes semblables sont aussi semblables par le sens au mot choisi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place as it character them you me yourself thirst bread course pity\n",
      "life love suffering uniform wantonness fever months enthusiasm\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "text.similar(\"affection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse à la question ici :\n",
    "# Pas incroyable ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** En utilisant la méthode `collocation_list`, veuilez afficher les 10 collocations (couples de mots) les plus fréquentes dans votre texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Katerina', 'Ivanovna'),\n",
       " ('Pyotr', 'Petrovitch'),\n",
       " ('Pulcheria', 'Alexandrovna'),\n",
       " ('Avdotya', 'Romanovna'),\n",
       " ('Rodion', 'Romanovitch'),\n",
       " ('Marfa', 'Petrovna'),\n",
       " ('Sofya', 'Semyonovna'),\n",
       " ('old', 'woman'),\n",
       " ('Porfiry', 'Petrovitch'),\n",
       " ('Amalia', 'Ivanovna')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "# nltk.download('stopwords')\n",
    "text.collocation_list(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e.** On peut déterminer le vocabulaire d'un texte (c'est-à-dire la liste de ses _types_) simplement en convertissant la liste des _tokens_ déjà obtenue dans une variable de type `set` en Python.  \n",
    "* Veuillez obtenir ainsi le vocabulaire de votre texte.\n",
    "* Combien de mots différents y a-t-il dans le vocabulaire, en incluant les ponctuations et tout autre symbole ? \n",
    "* Quels sont les 20 types les plus longs ? Que pensez-vous du résultat trouvé ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens uniques : 11011\n",
      "Nombre de tokens uniques (hors stopwords) : 10864\n",
      "20 premiers tokens uniques (hors stopwords) : ['grey-and-rainbow-coloured', 'frightened-looking', 'Schleswig-Holstein', 'disproportionately', 'well-proportioned', 'indistinguishable', 'cross-examination', 'Cough-cough-cough', '_psychologically_', 'enthusiastically', 'terrible-looking', '_Hof-kriegsrath_', 'pawnbroker-woman', 'misunderstanding', 'poverty-stricken', 'superciliousness', 'self-confidently', 'delicate-looking', 'governor-general', 'disproportionate']\n",
      "Nombre de mots composés : 13\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "token_set = set(all_tokens)\n",
    "print(\"Nombre de tokens uniques :\", len(token_set))\n",
    "token_not_in_stopwords = token_set - set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"Nombre de tokens uniques (hors stopwords) :\", len(token_not_in_stopwords))\n",
    "vocab_sorted_by_length = sorted(token_not_in_stopwords, key=lambda w: len(w), reverse=True)\n",
    "top_20_tokens = vocab_sorted_by_length[:20]\n",
    "print(\"20 premiers tokens uniques (hors stopwords) :\", top_20_tokens)\n",
    "print(\"Nombre de mots composés :\", len([w for w in top_20_tokens if \"-\" in w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse à la question ici :\n",
    "# La plupart des mots ce sont des noms composés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3f.** Veuillez construire la distribution de fréquences de votre texte, en utilisant un objet `FreqDist`, et utilisez-là pour afficher les mots qui ont plus de 4 lettres parmi les 70 mots les plus fréquents.\n",
    "\n",
    "Indication : NLTK peut facilement calculer les fréquences de tous les _tokens_ dans un `nltk.Text` en les stockant dans un objet de type `FreqDist` (pour _frequency distribution_) comme expliqué dans la [section 3.1 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#frequency-distributions).  À partir de cet objet, on peut déterminer les mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 tokens les plus fréquents : [(',', 16004), ('.', 8754), ('the', 7243), ('and', 6182), ('to', 5174), ('I', 4392), ('a', 4391), ('’', 4034), ('“', 3971), ('”', 3922), ('of', 3704), ('he', 3510), ('you', 3420), ('that', 3038), ('in', 3020), ('it', 2889), ('was', 2780), ('!', 2363), ('?', 2277), ('his', 1977), ('at', 1928), ('her', 1768), ('s', 1745), ('not', 1736), ('with', 1671), ('for', 1573), ('had', 1569), ('him', 1560), ('on', 1417), ('is', 1364), ('she', 1249), ('He', 1231), ('all', 1208), ('as', 1133), ('t', 1132), ('but', 1110), ('be', 1108), ('have', 1103), ('...', 1083), (';', 1046), ('me', 1040), ('are', 815), ('so', 792), ('Raskolnikov', 782), ('what', 751), ('And', 747), ('my', 742), ('were', 700), ('from', 696), ('But', 683), ('....', 683), ('out', 671), ('--', 663), ('up', 640), ('your', 628), ('they', 597), ('there', 597), ('You', 590), ('them', 583), ('this', 579), ('been', 576), ('one', 574), ('would', 552), ('The', 535), ('am', 532), ('an', 529), ('will', 525), ('about', 522), ('by', 521), ('know', 521)]\n",
      "Mots de plus de 4 caractères parmi les 70 tokens les plus fréquents : ['Raskolnikov', 'there', 'would', 'about']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "most_common_70 = freq_dist.most_common(70)\n",
    "print(\"70 tokens les plus fréquents :\", most_common_70)\n",
    "\n",
    "words_top_70_4_chars = [word for word, count in most_common_70 if len(word) > 4]\n",
    "print(\"Mots de plus de 4 caractères parmi les 70 tokens les plus fréquents :\", words_top_70_4_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graphiques\n",
    "La librairie `matplotlib` permet d'afficher les statistiques des textes sous forme graphique.  Pour la rendre accessible dans le _notebook_ (une fois installée dans Python), il faut exécuter les deux lignes suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a.** Veuillez afficher le graphique cumulatif des nombres d'occurrences des 70 mots les plus fréquents de votre texte, en vous inspirant de la  [section 3.1 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#frequency-distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez construir une liste avec la longueur de chaque _token_ du texte.  La liste aura donc autant de nombres que de tokens.  Veuillez créer un nouvel objet `FreqDist` à partir de cette liste de nombres, et affichez la distribution _non-cumulative_ des nombres d'occurrences.  Quelle est la longueur la plus fréquente ?  Comment évolue la longueur en fonction de la fréquence ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos réponses aux deux questions ici :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez générer la liste des fréquences des mots de votre texte par ordre décroissant (sans les mots, seulement les valeurs des fréquences).  Limitez cette liste à *N* valeurs (par exemple *N* = 100).  Affichez avec `matplotlib.pyplot.plot` la courbe en fonction du rang, c'est-à-dire le rang (1, 2, 3, ..., **) sur l'axe *x* et la fréquence sur l'axe *y*.\n",
    "\n",
    "Note : on génère directement des graphiques à partir de deux listes `x_values` et `y_values` avec la commande `matplotlib.pyplot.plot(x_values, y_values)`.\n",
    "\n",
    "Ajoutez une deuxième courbe (dans la même commande `plot`) selon la formule *y* = *a* / (*x* + *b*) en choississant par essais successifs des valeurs de *a* et *b* qui vous rapprochent autant que possible de la courbe des fréquences.  Cette formule est appelée *Loi de Zipf* et illustre une propriété du vocabulaire d'un échantillon suffisamment grand de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin de la partie 1b\n",
    "Veuillez nettoyer autant que possible ce _notebook_, exécutez une dernière fois toutes les cellules pour obtenir les résultats demandés, et enregistrez le _notebook_ en ajoutant vos deux noms.  Puis ajoutez-le dans une archive _zip_ avec le _notebook_ de la partie 1c, et soumettez l'archive sur Cyberlearn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CoursTAL",
   "language": "python",
   "name": "courstal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
